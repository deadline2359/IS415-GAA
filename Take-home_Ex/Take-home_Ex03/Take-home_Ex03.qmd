---
title: "Take-home Exercise 3"
format: 
  html:
    code-line-numbers: true
    number-sections: true
    highlight-style: github
execute:
  warning: false
  echo: true  
  eval: true
editor: visual
date: "18 March 2023"
date-modified: "26 March 2023"
---

```{css, echo=FALSE}
.panel-tabset .nav-link {
  background-color: #a3d2e3;
  box-shadow: 8px 5px 5px darkgrey;
}

.panel-tabset .tab-content{
  box-shadow: 8px 5px 5px darkgrey;
}
```

# Introduction

Around 77.9% of dwellings in Singapore are made up of Housing and Development Board (HDB) flats, public housing many residents are familiar with and take pride in.

Though the number of households living in HDB apartments continues to increase, there is a steady fall in number of Singaporeans actually living in these flats. One [argument](https://www.straitstimes.com/singapore/singapore-resident-population-in-hdb-flats-falls-to-304m-with-smaller-households-spread) provided is that more Singaporeans are selling their flats and "upgrading" to private housings, like condominium and private landed properties. Among those upgrading, selling their current house to afford for a better HDB apartments is not uncommon too.

On the flip side, there is the demand and there are many reasons for residents to buy resale flats.

1.  Families may not be keen to wait approximately [2 to 5.9 years](https://www.straitstimes.com/singapore/housing/how-are-bto-flats-built-and-why-do-waiting-times-vary-so-much) for an apartment under the Build-to-Order exercise.
2.  Couples, being [both Permanent Residents](https://www.propertyguru.com.sg/property-guides/how-do-singapore-permanent-residents-buy-a-hdb-flat-9914), can only access the resale market, if they want to buy a HDB flat.
3.  [Single citizens](https://www.hdb.gov.sg/residential/buying-a-flat/flat-and-grant-eligibility/singles) similarly are only allowed to purchase resale flats.

In spite of the ever increasing resale prices, [increasing 8.7% YoY](https://www.straitstimes.com/singapore/housing/hdb-resale-prices-rise-for-31st-straight-month-in-jan-sales-volume-rebounds-after-property-curbs#:~:text=Prices%20were%20up%208.7%20per,up%20by%205.4%20per%20cent.), and much more expensive than a BTO flat, resale flats continue to be in the cards for residents to buy their dream homes.

With how expensive these apartments are, you better bet that people will be choosing where to place their money more carefully.

With 3-room flats having a more diverse demographics of buyers, I would like to focus on this particular flat type in this exercise.

# Import Packages

Below the packages that we will be using in this exercise:

```{r}
pacman::p_load(sf, tmap, sfdep, tidyverse, olsrr, ggpubr, GWmodel, SpatialML, tidymodels, jsonlite, readxl, Rfast, corrplot, gtsummary, spdep, Metrics, ggplot) 
```

# Import Data

| Dataset                          | Source                                                                                       |
|----------------------------------|----------------------------------------------------------------------------------------------|
| HDB Resale Flat Prices           | [Data.gov.sg](https://data.gov.sg/dataset/resale-flat-prices)                                |
| Masterplan 2019 Subzone Boundary | Professor Kam Tin Seong                                                                      |
| Hawker Centres                   | [OpenMapSG](https://www.onemap.gov.sg/main/v2/)                                              |
| Elderly Centres                  | [OpenMapSG](https://www.onemap.gov.sg/main/v2/)                                              |
| Parks                            | [OpenMapSG](https://www.onemap.gov.sg/main/v2/)                                              |
| Kindergartens                    | [OpenMapSG](https://www.onemap.gov.sg/main/v2/)                                              |
| Childcare Centres                | [OpenMapSG](https://www.onemap.gov.sg/main/v2/)                                              |
| Bus Stops                        | [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)             |
| MRT Stations                     | [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)             |
| Primary Schools                  | [OpenMapSG](https://www.onemap.gov.sg/main/v2/)                                              |
| Supermarkets                     | [OpenMapSG](https://www.onemap.gov.sg/main/v2/)                                              |
| Malls                            | [Wikipedia](https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore)               |
| HDB MUP/HIP Status               | [Housing Development Board](https://services2.hdb.gov.sg/webapp/BB33RESLSTATUS/BB33SEnquiry) |

## Import HDB Resale Flat Prices

::: panel-tabset
### Code

```{r}
resale_prices <- read_csv("data/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv")
```

### Data

```{r}
head(resale_prices, 5)
```
:::

#### Convert "Month" to DataTime Format

As we want to have the abilities to filter the data according to its dates, it's best for us to convert the *date* column to a DateTime format.

::: panel-tabset
##### Code

```{r}
resale_prices$date <- zoo::as.Date(zoo::as.yearmon(resale_prices$month))
```

##### Data

```{r}
head(resale_prices$date, 5)
```
:::

##### Data - Filter Jan 2021 to Feb 2023

Here, we will filter the data so we will only be working with those from January 2021 to February 2023.

::: panel-tabset
##### Code

```{r}
resale_prices_total <- resale_prices %>% filter(date >= as.Date('2021-01-01')) %>% filter(date <= as.Date('2023-02-01'))
```

##### Data

```{r}
head(resale_prices_total, 5)
```
:::

## Import Subzone Data

::: panel-tabset
### Code

```{r}
subzone_sf <- st_read(dsn="data/geospatial/MPSZ-2019", layer="MPSZ-2019") %>% st_transform(crs = 3414)
```

### Data

```{r}
head(subzone_sf)
```
:::

## Import Independent Variables

With any creation of a predictive model, comes the independent variables that the predictions will rely on.

### Geospatial Datasets

Since this project is pertain to Singapore, OpenMapSG will be helpful for us to get these geospatial datasets with ease.

As you can see below, we will be using a package called *onemapsgapi* that wraps OpenMapSG API.

Using it is relatively simple:

1.  One will need to register an account with OpenMapSG
2.  Get a token using `get_token()`
3.  Search for datasets they want in `search_themes()`
4.  Optionally, look up on the status on desired dataset through `get_theme_status()`
5.  And finally get your data from `get_theme()`!

```{r eval=FALSE}
library(onemapsgapi)
token <- get_token("Your Email", "Your Password")
search_themes(token, "searchval")
get_theme_status(token, "themename")
themetibble <- get_theme(token, "themename")
```

```{r, eval=FALSE, echo=FALSE}
token <- get_token("Your Email", "Your Password")

search_themes(token)
get_theme_status(token, "polyclinic")
getAllThemesInfo(token)
```

::: callout-note
I will have already performed the above steps to acquire my datasets. In our case, I will directly read the geospatial data I have downloaded using `st_read()`.
:::

::: panel-tabset
#### Hawker Centres

```{r eval=FALSE, echo=FALSE}
hawkercentre_tibble <- onemapsgapi::get_theme(token, "hawkercentre")
hawkercentre_sf <- st_as_sf(hawkercentre_tibble, coords=c("Lng", "Lat"), crs=4326) 
st_write(hawkercentre_sf, "data/geospatial/hawkercentre/hawkercentre.shp")
```

```{r}
hawkercentre_sf <- st_read(dsn="data/geospatial/hawkercentre", layer="hawkercentre") %>% st_transform(crs = 3414)
```

#### Eldercare Centres

```{r eval=FALSE, echo=FALSE}
eldercare_tibble <- onemapsgapi::get_theme(token, "eldercare")
eldercare_sf <- st_as_sf(eldercare_tibble, coords=c("Lng", "Lat"), crs=4326)
st_write(eldercare_sf, "data/geospatial/eldercare.shp")
```

```{r}
eldercare_sf <- st_read(dsn="data/geospatial/eldercare", layer="eldercare") %>% st_transform(crs = 3414)
```

#### Parks

```{r eval=FALSE, echo=FALSE}
parks_tibble <- onemapsgapi::get_theme(token, "nationalparks")
parks_sf <- st_as_sf(parks_tibble, coords=c("Lng", "Lat"), crs=4326)
st_write(parks_sf, "data/geospatial/nationalparks.shp")
```

```{r}
parks_sf <- st_read(dsn="data/geospatial/nationalparks", layer="nationalparks") %>% st_transform(crs = 3414)
```

#### Kindergartens

```{r eval=FALSE, echo=FALSE}
kindergartens_tibble <- onemapsgapi::get_theme(token, "kindergartens")
kindergartens_sf <- st_as_sf(kindergartens_tibble, coords=c("Lng", "Lat"), crs=4326)
st_write(kindergartens_sf, "data/geospatial/kindergartens.shp")
```

```{r}
kindergartens_sf <- st_read(dsn="data/geospatial/kindergartens", layer="kindergartens") %>% st_transform(crs = 3414)
```

#### Childcare Centres

```{r eval=FALSE, echo=FALSE}
childcare_tibble <- onemapsgapi::get_theme(token, "childcare")
childcare_sf <- st_as_sf(childcare_tibble, coords=c("Lng", "Lat"), crs=4326)
st_write(childcare_sf, "data/geospatial/childcare.shp")
```

```{r}
childcare_sf <- st_read(dsn="data/geospatial/childcare", layer="childcare") %>% st_transform(crs = 3414)
```

#### Bus Stop

```{r}
busstop_sf <- st_read(dsn="data/geospatial/BusStop_Feb2023", layer="BusStop") %>% st_transform(crs = 3414)
```

#### MRT Stations

```{r}
mrt_sf <- st_read(dsn="data/geospatial/TrainStation_Feb2023", layer="RapidTransitSystemStation") %>% st_transform(crs = 3414)
```

#### Supermarkets

```{r}
supermarkets_sf <- st_read(dsn="data/geospatial/supermarkets", layer="SUPERMARKETS") %>% st_transform(crs = 3414)
```
:::

### Aspatial Data

::: panel-tabset
#### Primary Schools

```{r}
primarySch <- fromJSON("data/geospatial/primaryschools/primaryschools.json")[["SearchResults"]][-1,c(2:5, 7, 13:14)]
primarySch_sf <- st_as_sf(primarySch, coords=c("LONGITUDE", "LATITUDE"), crs=4326) %>% st_transform(crs = 3414)
```

#### Malls

```{r}
malls <- read_xlsx("data/aspatial/malls-20230320.xlsx")
```

#### HDB Upgrades

```{r}
hdb_upgrades <- read_xlsx("data/aspatial/HDB_HIP-MUP-20230312.xlsx")
```
:::

# Data Wrangling

## Central Business District

```{r}
cbd_sf <- subzone_sf %>% filter(subzone_sf$PLN_AREA_N == "DOWNTOWN CORE")
```

## Primary Schools

### Scraping of Ranking Data

To make full use of primary schools as an independent variable, we need to merge the ranking we get from [Schlah's Primary School Rankings](https://schlah.com/primary-schools). It provides substantial data in how it derived its ranking, which offers a more objective rank.

We then scraped this data from the website.

::: panel-tabset
#### Code

```{r}
primarySch_ranking <- fromJSON("data/geospatial/primaryschools/primaryschools_rankings.json")[["props"]][["pageProps"]][["sortedSchools"]]
```

#### Data

```{r}
head(primarySch_ranking, 5)
```
:::

### Wrangle Dataset on Primary Schools' Rankings

You may realise from above that **primarySch_ranking** that the there are quite a few columns prefixed with "schoolInfo\$", which mean that these columns are actually under the column **schoolInfo** in **primarySch_ranking**. Using `unnest()`, the columns inside **schoolInfo** will be expanded so that we can more easily access schools' informations like names, addresses.

::: panel-tabset
#### Code

```{r}
primarySch_ranking = unnest(primarySch_ranking, schoolInfo)[,c(1, 5, 9, 14:16, 22, 28:32)] 
```

#### Data

```{r}
head(primarySch_ranking)
```
:::

### Merging Geospatial and Aspatial Data of Primary Schools

Using postal codes, we will merge them together!

::: panel-tabset
#### Code

```{r}
primarySch_ranking_sf = merge(primarySch_ranking, primarySch_sf, by.x='postalCode', by.y="SCH_POSTAL_CODE")
```

#### Data

```{r}
head(primarySch_ranking_sf, 5)
```
:::

But sadly the merged sf has 180 rows while the original geospatial dataset has 181 rows, meaning there is a school with mismatch data.

##### Finding Mismatched Row

We will first order **primarySch_ranking_sf** according to their ranking.

```{r}
primarySch__order_ranking_sf = primarySch_ranking_sf[order(primarySch_ranking_sf$rank),]
```

We then loop through the **primarySch_ranking**, the original DataFrame, to find the schools not in the merged sf.

Note that there are actually 186 rows in **primarySch_ranking**, meaning there is a difference of 6 rows.

```{r}
schools_dont_exist = list()
for (i in 1:nrow(primarySch_ranking)){
  if (i %in% primarySch__order_ranking_sf$rank == FALSE){
    schools_dont_exist <- append(schools_dont_exist, i)
  }
}

schools <- data.frame()

for (sch in schools_dont_exist){
  schools = rbind(schools, primarySch_ranking[primarySch_ranking$rank == sch,])
}
schools
```

So... we need to look through all the current situations of all 6 schools and determine, which one is the one in OneMapSG's geospatial data.

-   **Pioneer Primary School** merged in **Juying Primary School** in 2021. In addition, the merged school is moving to a new location in 2026, hence not opening from its Primary 1 Registration from 2021 to 2024
-   **Stamford Primary School** has merged with Farrer Park Primary School in 2023.
-   **Eunos Primary School** has merged with Telok Kurau Primary School.
-   **Guangyang Primary School** has merged with Townsville Primary School.

With that, **Angsana Primary School** is the row that has its postal code different.

Checking the school's website, **Angsana Primary School**'s postal code is actually 528565. Hence we need to change **primarySch_ranking**'s postal code to facilitate merging.

```{r}
primarySch_ranking[primarySch_ranking$schoolName == "Angsana Primary School","postalCode"] = '528565'

primarySch_ranking_df = merge(primarySch_ranking, primarySch_sf, by.x='postalCode', by.y="SCH_POSTAL_CODE")
nrow(primarySch_ranking_df)
primarySch_ranking_sf <- st_as_sf(primarySch_ranking_df) %>% st_transform(crs = 3414)
```

## Masterplan Subzone 2019

Looking at the subzone dataset, it seems that there are invalid geometries in it.

```{r}
length(which(st_is_valid(subzone_sf) == FALSE))
```

Thus, we will be using `st_make_valid()` to correct them.

```{r}
subzone_sf <- st_make_valid(subzone_sf)
length(which(st_is_valid(subzone_sf) == FALSE))
```

## MRT

Similarly for MRT, there are invalid geometries as well.

```{r}
length(which(st_is_valid(mrt_sf) == FALSE))
```

```{r}
mrt_sf[st_is_valid(mrt_sf) == FALSE,]
```

However, some geometries have less than 4 polygons, which `st_make_valid()` does not resolve. Thus we will be excluding them from our exercise.

```{r}
mrt_sf <- mrt_sf[st_is_valid(mrt_sf) == TRUE,]
mrt_sf <- mrt_sf[!st_is_empty(mrt_sf),,drop=FALSE]
length(which(st_is_valid(mrt_sf) == FALSE))
```

## HDB

The sf DataFrame we are acquired from Data.gov.sg does not have any geospatial data. But it has the block and streetname that we can derived the data from, through OpenMapSG API.

```{r}
library(httr)
geocode <- function(block, streetname="") {
  base_url <- "https://developers.onemap.sg/commonapi/search"
  address <- paste(block, streetname, sep = " ")
  query <- list("searchVal" = address, 
                "returnGeom" = "Y",
                "getAddrDetails" = "N",
                "pageNum" = "1")
  
  res <- GET(base_url, query = query)
  restext<-content(res, as="text")
  
  output <- fromJSON(restext)  %>% 
    as.data.frame %>%
    select(results.LATITUDE, results.LONGITUDE)

  return(output)
}
```

::: panel-tabset
### Get Coordinates

```{r eval=FALSE}
resale_prices_total$LATITUDE <- 0
resale_prices_total$LONGITUDE <- 0

for (i in 1:nrow(resale_prices_total)){
  temp_output <- geocode(resale_prices_total[i, 4], resale_prices_total[i, 5])
  
  resale_prices_total$LATITUDE[i] <- temp_output$results.LATITUDE
  resale_prices_total$LONGITUDE[i] <- temp_output$results.LONGITUDE
}
```

### Write to SHP

```{r eval=FALSE}
resale_prices_sf <- st_as_sf(resale_prices_total, coords=c("LONGITUDE", "LATITUDE"), crs=4326)
st_write(resale_prices_sf, "data/geospatial/resale_prices/resale_prices.shp")
```

### Read SHP

```{r}
resale_prices_sf <- st_read(dsn="data/geospatial/resale_prices", layer="resale_prices") %>% st_transform(crs = 3414)
```
:::

### Merge with HDB Status

We also have data on the upgrades done on these buildings. It will be good to separate the both two types of upgrades.

Note that MUP is actually then the precessor of HIP before 2007. Differentiating the upgrades then ables us to see if the timing of upgrades has an impact on the resell prices.

#### Home Improvement Programme (HIP)

HIP is described as HDB's programme to "resolve common maintenance problems of ageing flats".

```{r}
hdb_hip <- hdb_upgrades %>% filter(TYPE == "HIP")
colnames(hdb_hip)[3] ="HIP"
```

```{r}
resale_prices_upgrade_sf <- merge(resale_prices_sf, hdb_hip, by.x=c("block", "strt_nm"), by.y=c("BLK", "STREET"), all.x = TRUE)
```

#### Main Upgrading Programme (MUP)

Before 2007, MUP was dedicated to do the same things.

```{r}
hdb_mup <- hdb_upgrades %>% filter(TYPE == "MUP")
colnames(hdb_mup)[3] ="MUP"
```

```{r}
resale_prices_upgrade_sf <- merge(resale_prices_upgrade_sf, hdb_mup, by.x=c("block", "strt_nm"), by.y=c("BLK", "STREET"), all.x = TRUE)
```

## Malls

Finally, we have the mall's data from Wikipedia. Similar to the HDB flats, we don't have geospatial data. But thankfully, we can do the same thing using OneMapSG to get the coordinates.

::: panel-tabset
### Get Coordinates

```{r eval=FALSE}
malls$LATITUDE <- 0
malls$LONGITUDE <- 0

for (i in 1:nrow(malls)){
  temp_output <- geocode(malls[i, 1], )
  
  malls$LATITUDE[i] <- temp_output$results.LATITUDE
  malls$LONGITUDE[i] <- temp_output$results.LONGITUDE
}
```

### Write to SHP

```{r eval=FALSE}
malls_sf <- st_as_sf(malls, coords=c("LONGITUDE", "LATITUDE"), crs=4326)
st_write(malls_sf, "data/geospatial/malls/malls.shp")
```

### Read SHP

```{r}
malls_sf <- st_read(dsn="data/geospatial/malls", layer="malls") %>% st_transform(crs = 3414)
```
:::

## Top Primary Schools

It is difficult to really determine what is the appropriate number of top schools to filter for, since schools can very much have their own specialties and histories (like alumni communities) that can affect the data, outside of the ones used by the website we scrapped the ranking data from.

Hence top 20 is admittedly derived after I looked through the dataset and judged that the top 20 schools are really known to be some of the best primary schools, which sentiments I believe will be similar among the general Singapore population.

```{r}
top_primarySch_ranking_sf <- primarySch_ranking_sf %>% filter(rank <= 20)
```

# Proximity

There are two parts to proximity. One being the distance from specific locations and another being the number of such locations within its proximity.

## Proximity - Distance

Below is the function that enables us to attain the distance between a HDB apartment and specific facilities.

```{r eval=FALSE}
proximity_calculator <- function(original_sf, derived_sf, column_name) {
  dist_matrix <- st_distance(original_sf, derived_sf)
  original_sf[, column_name] <- rowMins(dist_matrix) / 1000
  return(original_sf)
}
```

```{r eval=FALSE}
resale_proximity_sf <- resale_prices_upgrade_sf 
resale_proximity_sf <- proximity_calculator(original_sf = resale_proximity_sf, derived_sf = cbd_sf, column_name = "PROX_CBD")
resale_proximity_sf <- proximity_calculator(original_sf = resale_proximity_sf, derived_sf = hawkercentre_sf, column_name="PROX_HAWKER")
resale_proximity_sf <- proximity_calculator(original_sf = resale_proximity_sf, derived_sf = eldercare_sf, column_name = "PROX_ELDERCARE")
resale_proximity_sf <- proximity_calculator(original_sf = resale_proximity_sf, derived_sf = parks_sf, column_name="PROX_PARK")
resale_proximity_sf <- proximity_calculator(original_sf = resale_proximity_sf, derived_sf = kindergartens_sf, column_name = "PROX_KINDERGARTEN")
resale_proximity_sf <- proximity_calculator(original_sf = resale_proximity_sf, derived_sf = childcare_sf, column_name = "PROX_CHILDCARE")
resale_proximity_sf <- proximity_calculator(original_sf = resale_proximity_sf, derived_sf = busstop_sf, column_name = "PROX_BUSSTOP")
resale_proximity_sf <- proximity_calculator(original_sf = resale_proximity_sf, derived_sf = mrt_sf, column_name = "PROX_MRT")
resale_proximity_sf <- proximity_calculator(original_sf = resale_proximity_sf, derived_sf = primarySch_ranking_sf, column_name = "PROX_SCH")
resale_proximity_sf <- proximity_calculator(original_sf = resale_proximity_sf, derived_sf = supermarkets_sf, column_name="PROX_SUPERMARKET")
resale_proximity_sf <- proximity_calculator(original_sf = resale_proximity_sf, derived_sf = malls_sf, column_name = "PROX_MALL")
resale_proximity_sf <- proximity_calculator(original_sf = resale_proximity_sf, derived_sf = top_primarySch_ranking_sf, column_name = "PROX_TOPSCH")
```

## Proximity - Number

Below is then the function that enables us to attain the number of such facilities within specific distances of a HDB apartment.

```{r eval=FALSE}
library(units)
radius_calculator <- function(original_sf, derived_sf, column_name, radius) {
  dist_matrix <- st_distance(original_sf, derived_sf) %>%
    drop_units() %>%
    as.data.frame()
  original_sf[,column_name] <- rowSums(dist_matrix <= radius)
  return(original_sf)
}
```

```{r eval=FALSE}
resale_proximity_sf <- radius_calculator(resale_proximity_sf, kindergartens_sf, "NUM_KINDERGARTEN", 350)
resale_proximity_sf <- radius_calculator(resale_proximity_sf, childcare_sf, "NUM_CHILDCARE", 350)
resale_proximity_sf <- radius_calculator(resale_proximity_sf, busstop_sf, "NUM_BUSSTOP", 350)
resale_proximity_sf <- radius_calculator(resale_proximity_sf, primarySch_ranking_sf, "NUM_SCH", 1000)
resale_proximity_sf <- radius_calculator(resale_proximity_sf, top_primarySch_ranking_sf, "NUM_TOPSCH", 1000)
```

## Save to SHP

::: panel-tabset
### Save to SHP

```{r eval=FALSE}
st_write(resale_proximity_sf, "data/geospatial/resale_proximity/resale_proximity.shp")
```

### Read SHP

```{r}
resale_proximity_sf <- st_read(dsn="data/geospatial/resale_proximity", layer="resale_proximity")
```
:::

# Encoding Data

To perform machine learning techniques in many circumstances requires us to encode the data so that the algorithms can understand and process the data for predictions.

In the below tabs, they represent the columns that have data in non-numeric datatypes.

::: panel-tabset
## Floor Level

```{r}
storey_category <- unique(resale_proximity_sf$stry_rn)
storey_category
```

```{r}
storey_names <- 1:length(storey_category)
storey_order <- data.frame(storey_category, storey_names)
storey_order
```

```{r}
resale_level_loc_sf <- merge(resale_proximity_sf, storey_order, by.x = "stry_rn", by.y = "storey_category", all.x=TRUE)
resale_level_loc_sf
```

## Remaining Lease

```{r eval=FALSE}
str_list <- str_split(resale_level_loc_sf$rmnng_l, " ")

for (i in 1:length(str_list)) {
  if (length(unlist(str_list[i])) > 2) {
      year <- as.numeric(unlist(str_list[i])[1])
      month <- as.numeric(unlist(str_list[i])[3])
      resale_level_loc_sf[i, "remaining_lease"] <- (year * 12) + month
  }
  else {
    year <- as.numeric(unlist(str_list[i])[1])
    resale_level_loc_sf[i, "remaining_lease"] <- (year * 12)
  }
}
resale_remaining_lease_loc_sf <- resale_level_loc_sf
```

```{r eval=FALSE}
st_write(resale_remaining_lease_loc_sf, "data/geospatial/resale_proximity/resale_proximity.shp")
```

```{r}
resale_remaining_lease_loc_sf <- st_read(dsn="data/geospatial/resale_proximity", layer="resale_proximity") %>% st_transform(crs = 3414)
```

## Age of Unit

```{r}
resale_remaining_lease_loc_sf$year <- zoo::as.Date(zoo::as.yearmon(resale_remaining_lease_loc_sf$ls_cmm_))
resale_remaining_lease_loc_sf$age = round(interval(resale_remaining_lease_loc_sf$year, resale_remaining_lease_loc_sf$date)/ years(1))
resale_age_loc_sf <- resale_remaining_lease_loc_sf
```

## Upgrades

::: panel-tab
### Encoding

```{r}
resale_upgrade_loc_sf <- resale_age_loc_sf
```

```{r eval=FALSE}
resale_upgrade_loc_sf$HIP <- ifelse(is.na(resale_upgrade_loc_sf$HIP), 0, 1)
resale_upgrade_loc_sf$MUP <- ifelse(is.na(resale_upgrade_loc_sf$MUP), 0, 1)
```

### Write to SHP

```{r eval=FALSE}
st_write(resale_remaining_lease_loc_sf, "data/geospatial/resale_upgrade_loc_sf/resale_upgrade_loc_sf.shp")
```

### Read SHP

```{r}
resale_proximity_sf <- st_read(dsn="data/geospatial/resale_upgrade_loc_sf", layer="resale_upgrade_loc_sf")
```
:::

## HDB Types

```{r}
hdb_types <- unique(resale_upgrade_loc_sf$flt_typ)
hdb_types
```

```{r}
resale_type_loc_sf <- resale_upgrade_loc_sf %>% filter(resale_upgrade_loc_sf$flt_typ == "3 ROOM")
resale_type_loc_sf
```

## Save to RDS

::: panel-tabset
### Rename Columns

```{r}
final_resale_prices <- resale_type_loc_sf
final_resale_prices <- final_resale_prices %>% 
        rename("storey_range" = "stry_rn",
               "street_name" = "strt_nm",
               "flat_type" = "flt_typ",
               "floor_area_sqm" = "flr_r_s",
               "flat_model" = "flt_mdl",
               "lease_commence_date" = "ls_cmm_",
               "remaining_lease_original" = "rmnng_l",
               "resale_price" = "rsl_prc",
               "PROX_CBD" = "PROX_CB",
               "PROX_HAWKER" = "PROX_HA",
               "PROX_ELDERCARE" = "PROX_EL",
               "PROX_KINDERGARTEN" = "PROX_KI",
               "PROX_CHILDCARE" = "PROX_CH",
               "PROX_BUSSTOP" = "PROX_BU",
               "PROX_MRT" = "PROX_MR",
               "PROX_SCH" = "PROX_SC",
               "PROX_TOPSCH" = "PROX_TO",
               "PROX_SUPERMARKET" = "PROX_SU",
               "PROX_MALL" = "PROX_MA",
               "NUM_KINDERGARTEN" = "NUM_KIN",
               "NUM_CHILDCARE" = "NUM_CHI",
               "NUM_BUSSTOP" = "NUM_BUS",
               "NUM_TOPSCH" = "NUM_TOP")
names(final_resale_prices)
```

### Write to RDS

```{r eval=FALSE}
write_rds(final_resale_prices, "data/model/final_resale_prices.rds")
```

### Read RDS

```{r}
final_resale_prices <- read_rds("data/model/final_resale_prices.rds")
```
:::
:::

# Exploratory Data Analysis (EDA)

Here, we want to view and understand the data as it is before proceeding with modelling.

::: callout-note
The data below used are for those transactions within January 2021 to February 2023, and for only 3-room apartment.
:::

## Statistical Graphics

## Distribution of Resale Prices

```{r}
library(patchwork)
price_3rm <- ggplot(data=final_resale_prices, aes(x=`resale_price`)) +
  geom_histogram(bins=20, color="black", fill="orange")

price_all <- ggplot(data=resale_prices_sf, aes(x=`rsl_prc`)) +
  geom_histogram(bins=20, color="black", fill="lightblue")

price_3rm + price_all
```

We can see that 3-room HDB flats lean to the right, like all flats. But noticeably, the prices are definitely lower than other sorts of flats, usually with more actual rooms.

## Distribution of Resale Prices

Looks like 4-room apartments have the most transactions. Whereas 5-room and 3-room flats come second and third respectively. There could be more interest in these sort of flats, but can also be due to most flats are 3 to 5 room flats, which limits options available.

```{r}
ggplot(resale_prices_sf, aes(flt_typ)) +
  geom_bar(fill="orange")
```

# Preparation for Modelling

We will now move on to model preparation.

## Filtering

### Train

Sadly due to my laptop's limited computation power, I have filtered the training data to be only from June 2022 to December 2022 :(

We will also filter for the columns that are of interest to us, as shown below.

```{r}
names(final_resale_prices)[c(7, 11, 13, 15, 17:34, 36)]
```

```{r}
train_resale_prices <- final_resale_prices %>% filter(between(date, as.Date('2022-06-01'), as.Date('2022-12-01')))
train_resale_prices <- train_resale_prices[, c(7, 11, 13, 15, 17:34, 36)]
```

### Test

The test data is then this year's transactions, being in January and February 2023.

```{r}
test_resale_prices <- final_resale_prices %>% filter(between(date, as.Date('2023-01-01'), as.Date('2023-02-01')))
test_resale_prices <- test_resale_prices[, c(7, 11, 13, 15, 17:34, 36)]
```

## Visualising the relationships of the independent variables

We will want to check for **multicollinearity**, in that whether any of the variables is highly correlated with one another. If not removed, the models may not be wholy representative of the different variables and the predicted hence then be wrong.

Correlation matrix is what we will be using in this section.

```{r}
total_resale_prices <- final_resale_prices[, c(7, 11, 13, 15, 17:34, 36)]
st_geometry(total_resale_prices) <- NULL
```

```{r}
corrplot(cor(total_resale_prices), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

Looks like age of the HDB flats and remaining lease are practically inversely correlated, having a correlation of more than +/-0.8. So one will be removed, age will be removed.

Similarly, proximity of schools will be removed in favour of top schools.

# OLS Multiple Linear Regression (MLR) Model

`lm()` is applied to calibrate our MLR model.

```{r}
resale_price.mlr <- lm(formula = resale_price ~ floor_area_sqm + HIP + MUP + PROX_CBD + PROX_HAWKER +  PROX_ELDERCARE + PROX_KINDERGARTEN + PROX_CHILDCARE + PROX_BUSSTOP + PROX_MRT +  PROX_SUPERMARKET + PROX_MALL + PROX_TOPSCH + NUM_KINDERGARTEN + NUM_CHILDCARE + NUM_BUSSTOP + NUM_TOPSCH, data = train_resale_prices)
summary(resale_price.mlr)
```

## Preparation of Publication Quality Table

Using `tbl_regression()`, we want to find out which variables are not significant significant enough to retain in the model.

```{r}
tbl_regression(resale_price.mlr, intercept = TRUE)
```

With PROX_ELDERCARE, PROX_KINDERGARTEN, PROX_CHILDCARE, PROX_MRT and NUM_BUSSTOP having a p-value above 0.01, we will subsequently remove them to improve our model.

::: panel-tabset
## Version 1: Create New MLR

```{r}
resale_price.mlr <- lm(formula = resale_price ~ floor_area_sqm + HIP + MUP + PROX_CBD + PROX_HAWKER +  + PROX_BUSSTOP +  PROX_SUPERMARKET + PROX_MALL + PROX_TOPSCH + NUM_KINDERGARTEN + NUM_CHILDCARE + NUM_TOPSCH, data = train_resale_prices)
```

## Check for Significance

```{r}
tbl_regression(resale_price.mlr, intercept = TRUE)
```
:::

## Save to RDS

```{r eval=FALSE}
write_rds(resale_price.mlr, "data/model/resale_price.mlr.rds")
```

## Read RDS

```{r}
resale_price.mlr <- read_rds("data/model/resale_price.mlr.rds")
summary(resale_price.mlr)
```

:::

Yes! Now, there are no more insignificant variables. Let's move on.

## Checking for Non-Linearity

With most datapoints scatter on and around the 0 line, we can determine the relationship is linear.

```{r}
ols_plot_resid_fit(resale_price.mlr)
```

## Checking for Normality Assumption

The model is also normal in distribution.

```{r}
ols_plot_resid_hist(resale_price.mlr)
```

## Checking for Spatial Autocorrelation

With this test, we then check for the spatial variation in resell prices using the residuals from our MLR model.

Because we need the geospatial polygons to map our residuals and that **spdep** package only processes sp datatype, we will first convert *resale_price.mlr* to a DataFrame, then to a sf DataFrame, finally a sp DataFrame.

```{r}
resale_price.mlr.output <- as.data.frame(resale_price.mlr$residuals)
final_resale_price.mlr.sf <- cbind(train_resale_prices, resale_price.mlr.output) %>%
  rename(`MLR_RES` = `resale_price.mlr.residuals`)
final_resale_price.mlr.sp <- as_Spatial(final_resale_price.mlr.sf)
final_resale_price.mlr.sp
```

### Visualisation

```{r}
tmap_mode("view")
tm_shape(subzone_sf)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(final_resale_price.mlr.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

From the visualisation, we can determine that there are signs of spatial autocorrelation.

## Prediction

Finally, we come to prediction.

::: panel-tabset
### Code

```{r}
mlr_pred <- predict.lm(resale_price.mlr, test_resale_prices)
```

### Write to RDS

```{r}
write_rds(mlr_pred, "data/model/mlr_pred.rds")
```

### Read RDS

```{r}
mlr_pred <- read_rds("data/model/mlr_pred.rds")
```
:::

```{r}
mlr_pred_df <- as.data.frame(mlr_pred)
test_data_mlr <- cbind(test_resale_prices, mlr_pred_df)
```

Using Root Mean Square Error (RMSE), we can observe the difference between the predicted values and the actual values, which enables us to determine whether the model fares well in predicting.

```{r}
Metrics::rmse(test_data_mlr$resale_price, 
     test_data_mlr$mlr_pred)
```

### Visualisation

```{r}
ols_scatterplot <- ggplot(data = test_data_mlr,
       aes(x = mlr_pred,
           y = resale_price)) +
  geom_point() +
  geom_abline(col = "Red")

ols_scatterplot
```

Although there is a visible trend in how the datapoints are structured (i.e., a line), you can see that most datapoints fall out this line. This suggests that the model is probably decent, but has way more areas for improvements in order to accurately predict future transactional prices.

# Geographical Random Forest Model (GRFW)

In this section, we are seeking to create a GRFM through an adaptive bandwidth approach

Since conducting the Random Forest model will require us to drop the coordinates, we will first save them.

```{r}
coords_train <- st_coordinates(train_resale_prices)
coords_test <- st_coordinates(test_resale_prices)
```

```{r}
train_resale_prices <- train_resale_prices %>% 
  st_drop_geometry()

train_resale_prices <- train_resale_prices %>% 
  st_drop_geometry()
```

```{r eval=FALSE}
resale_price.sp <- as_Spatial(train_resale_prices)
```

## Optimal Bandwidth

Since we are creating an adaptive model, we will use `bw.gwr()` from **GWmodel** package to determine the optimal bandwidth.

```{r eval=FALSE}
set.seed(1234)
bw_adaptive <- bw.gwr(resale_price ~ floor_area_sqm + HIP + MUP + PROX_CBD + PROX_HAWKER +  PROX_ELDERCARE + PROX_KINDERGARTEN + PROX_CHILDCARE + PROX_BUSSTOP + PROX_MRT +  PROX_SUPERMARKET + PROX_MALL + PROX_TOPSCH + NUM_KINDERGARTEN + NUM_CHILDCARE + NUM_BUSSTOP + NUM_TOPSCH, 
                      data=resale_price.sp, 
                      approach="CV", 
                      kernel="gaussian", 
                      adaptive=TRUE, 
                      longlat=FALSE)
```

![](data/img/bw_gwr.jpg){width="617"}

As you can see from above, the most suitable bandwidth is 208.

## Calibration of Geographical Random Forest Model (GRFM)

Using the **SpatialML** package, we calibrate our Random Forest model from there.

::: panel-tabset
### Calibration

```{r eval=FALSE}
set.seed(1234)
grf_adaptive <- grf(formula = resale_price ~ floor_area_sqm + HIP + MUP + PROX_CBD + PROX_HAWKER +  PROX_ELDERCARE + PROX_KINDERGARTEN + PROX_CHILDCARE + PROX_SUPERMARKET + PROX_MALL + PROX_TOPSCH + NUM_KINDERGARTEN + NUM_CHILDCARE + NUM_BUSSTOP + NUM_SCH + NUM_TOPSCH, 
                     dframe=train_resale_prices,
                     bw=bw_adaptive,
                     kernel="adaptive",
                     coords=coords_train,
                      ntree = 30)
```

### Save to RDS

```{r eval=FALSE}
write_rds(grf_adaptive, "data/model/grf_adaptive.rds")
```

### Read RDS

```{r}
grf_adaptive <- read_rds("data/model/grf_adaptive.rds")
```
:::

## Prediction

### Preparation

Now, we can combine the test data and coordinates together.

```{r}
test_data <- cbind(test_resale_prices, coords_test) %>%
  st_drop_geometry()
```

And onwards to our prediction using `predict.grf()` using the above variable.

::: panel-tabset
### Prediction

```{r}
gwRF_pred <- predict.grf(grf_adaptive, 
                           test_data, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

### Write to rDS

```{r}
gwRF_pred <- write_rds(gwRF_pred, "data/model/gwRF_pred.rds")
```

### Read RDS

```{r}
gwRF_pred <- read_rds("data/model/gwRF_pred.rds")
gwRF_pred_df <- as.data.frame(gwRF_pred)
```
:::

#### Visualisation

Like in the OLS model above, we will use calculate the RMSE to determine the accuracy of our new model.

```{r}
test_data_plot <- cbind(test_data, gwRF_pred)
```

```{r}
Metrics::rmse(test_data_plot$resale_price, 
     test_data_plot$gwRF_pred)
```

```{r}
ggplot(data = test_data_plot,
       aes(x = gwRF_pred,
           y = resale_price)) +
  geom_point() +
  geom_abline(col = "Red")
```

The chart showcases the datapoints scattered relatively straight across a line, suggesting predicted resale prices and actual resale prices are quite close together.

# Conclusion

In comparing the two different models, the Geographical Random Forest model perform way better than Linear Regression using OSL method, in predicting resell prices for 3-room flats. This is in line with what we know with how Linear Regression fails to take into account with some geospatial conditions like spatial autocorrelation.

Interesting, we eliminated childcare centres, kindergartens and proxity to MRT due to them not being significant enough to be included in the Multi-Linear Regression model, which is shocking personally. In the context of this exercise, it is not known whether there is a difference in significant variables among different flat types (e.g., 2-room). Personally, I may suspect that this is due to the diverse demographics (particularly singles) that purchase this sort of flat type, in contrast with other types.

# Acknowledges

-   [Megan Sim's Take-Home Exercise 3](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/)

-   Professor Kam Tin Seong's Exercises:

    -   [In-class Exercise 9](https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex09/in-class_ex09_gwml#calibrating-geographical-random-forest-model) -

    -   [R for Geospatial Data Science and Analytics Chapter 13](https://r4gdsa.netlify.app/chap13.html#building-adaptive-bandwidth-gwr-model)
